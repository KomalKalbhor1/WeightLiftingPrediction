{
  "name": "Weight Lifting prediction mini data-science exercise",
  "tagline": "Practical Machine Learning Write-up for predictions based on Weight Lifting Exercise Dataset",
  "body": "Practical Machine Learning Course Project: Weight Lifting prediction\r\n================\r\n__by Andrei Ionut Damian, email: damian@cloudifier.net __\r\n\r\nThe Short description\r\n-----------------\r\n\r\nThis page presents my whole approach for solving Practical Machine Learning Course Exercise from Johns Hopkins University Data Science Specialization on Coursera. Data Science and Practical Machine Learning creators are: \r\n**Roger D. Peng**, PhD, Associate Professor, Biostatistics; **Brian Caffo**, PhD, Professor, Biostatistics; **Jeff Leek**, PhD, Associate Professor, Biostatistics.\r\nIn this script I have used a specific approach for data exploration and cleaning and also my specific approach for model selection, training, cross-validation and finally testing. \r\n###Note: Some steps both in data exploration/cleaning and model training could have been omitted from this final \"transcript\" of my work, however I decided to include everything to present the approach, the order of the steps, the logical conclusions I derived from each step - even if some of those steps along the way have become partially or totally obsolete.  \r\n\r\nThe Background\r\n-----------------\r\nThis text is based on Coursera presentation of the present Data Science exercise:\r\n_Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement â€“ a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, your goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from the website here: http://groupware.les.inf.puc-rio.br/har (see the section on the Weight Lifting Exercise Dataset)_.\r\n\r\nThe approach\r\n-----------------\r\n## For data exploration:\r\n\r\n-   clean useless (useless in regard to prediction) data columns such as index, person names, etc\r\n-   clean **NA data predictor** variables\r\n-   clean **empty data** predictor variables\r\n-   near **zero variance predictor** variables\r\n-   predictor **variables correlation** analysis (pair feature plot excluded for this case)\r\n-   analyze **variables importance** based on Recursive Feature Elimination and choose best selection\r\n\r\n## For model preparation, selection, cross-validation\r\n\r\n-   from initial training dataset three subsets are generated: training, cross-validation and testing\r\n-   **optional** PCA is used to pre-process training data then PCA model is used to fit cross/testing\r\n-   there is option to either use or not PCA as well as use or not the automated selected features: for final run I chosed not to use PCA and used automate selected features (based on RFE)\r\n-   a **list** of models is used in a automated/iterative process\r\n-   each model is **cross** evaluated including running time\r\n\r\nCode sections\r\n-------------\r\n\r\nBegin with loading `caret` and also `parallel` processing so we can use all cores in order to speed up all the heavy computing tasks:\r\n\r\n``` r\r\navail_cores <- detectCores() \r\np_cluster <- makeCluster(avail_cores)\r\nregisterDoParallel(p_cluster)\r\nsprintf(\"Cores registered = %d\",getDoParWorkers())\r\n```\r\n\r\n    ## [1] \"Cores registered = 8\"\r\n\r\nLOADING AND CLEANING\r\n--------------------\r\n\r\nTo start we have to identify current script working directory then load all data in data exploration dataframe `exploreData` and make a copy in `finalData` that will be used later on for final cleaning process.\r\n\r\n``` r\r\nexploreData <- read.csv(\"pml-training.csv\")\r\nfinalData <- data.frame(exploreData)\r\n```\r\n\r\nFirst lets do a quick analysis of zero variance predictors with `nearZeroVar(exploreData, saveMetrics = TRUE)`. We will display the total amount of near-zero variance predictors and the head of the table containing them. We are going to maintain a list of `ALL DROPPED COLUMNS` in order to use it for the final test dataset pre-processing (together with the PCA model if any).\r\n\r\n    ## [1] \"Total number of to-be-dropped near-zero var features = 60\"\r\n\r\n    ##                       freqRatio percentUnique zeroVar  nzv\r\n    ## new_window             47.33005    0.01019264   FALSE TRUE\r\n    ## kurtosis_yaw_belt      47.33005    0.01019264   FALSE TRUE\r\n    ## skewness_yaw_belt      47.33005    0.01019264   FALSE TRUE\r\n    ## kurtosis_yaw_dumbbell  47.33005    0.01019264   FALSE TRUE\r\n    ## skewness_yaw_dumbbell  47.33005    0.01019264   FALSE TRUE\r\n    ## kurtosis_yaw_forearm   47.33005    0.01019264   FALSE TRUE\r\n\r\nNow it is obvious we have a lot of **cleaning** to do on data so we need to start the data exploration and cleaning process - first drop totally useless columns such as observation number, name and then start working on the NA columns, find if there are NA-only columns or columns with more than 95% NA, get na column indexes then finally get the actual column names and display them:\r\n\r\n    ## [1] \"Number of NA columns to-be-dropped = 67\"\r\n\r\n    ##  [1] \"max_roll_belt\"            \"max_picth_belt\"          \r\n    ##  [3] \"min_roll_belt\"            \"min_pitch_belt\"          \r\n    ##  [5] \"amplitude_roll_belt\"      \"amplitude_pitch_belt\"    \r\n    ##  [7] \"var_total_accel_belt\"     \"avg_roll_belt\"           \r\n    ##  [9] \"stddev_roll_belt\"         \"var_roll_belt\"           \r\n    ## [11] \"avg_pitch_belt\"           \"stddev_pitch_belt\"       \r\n    ## [13] \"var_pitch_belt\"           \"avg_yaw_belt\"            \r\n    ## [15] \"stddev_yaw_belt\"          \"var_yaw_belt\"            \r\n    ## [17] \"var_accel_arm\"            \"avg_roll_arm\"            \r\n    ## [19] \"stddev_roll_arm\"          \"var_roll_arm\"            \r\n    ## [21] \"avg_pitch_arm\"            \"stddev_pitch_arm\"        \r\n    ## [23] \"var_pitch_arm\"            \"avg_yaw_arm\"             \r\n    ## [25] \"stddev_yaw_arm\"           \"var_yaw_arm\"             \r\n    ## [27] \"max_roll_arm\"             \"max_picth_arm\"           \r\n    ## [29] \"max_yaw_arm\"              \"min_roll_arm\"            \r\n    ## [31] \"min_pitch_arm\"            \"min_yaw_arm\"             \r\n    ## [33] \"amplitude_roll_arm\"       \"amplitude_pitch_arm\"     \r\n    ## [35] \"amplitude_yaw_arm\"        \"max_roll_dumbbell\"       \r\n    ## [37] \"max_picth_dumbbell\"       \"min_roll_dumbbell\"       \r\n    ## [39] \"min_pitch_dumbbell\"       \"amplitude_roll_dumbbell\" \r\n    ## [41] \"amplitude_pitch_dumbbell\" \"var_accel_dumbbell\"      \r\n    ## [43] \"avg_roll_dumbbell\"        \"stddev_roll_dumbbell\"    \r\n    ## [45] \"var_roll_dumbbell\"        \"avg_pitch_dumbbell\"      \r\n    ## [47] \"stddev_pitch_dumbbell\"    \"var_pitch_dumbbell\"      \r\n    ## [49] \"avg_yaw_dumbbell\"         \"stddev_yaw_dumbbell\"     \r\n    ## [51] \"var_yaw_dumbbell\"         \"max_roll_forearm\"        \r\n    ## [53] \"max_picth_forearm\"        \"min_roll_forearm\"        \r\n    ## [55] \"min_pitch_forearm\"        \"amplitude_roll_forearm\"  \r\n    ## [57] \"amplitude_pitch_forearm\"  \"var_accel_forearm\"       \r\n    ## [59] \"avg_roll_forearm\"         \"stddev_roll_forearm\"     \r\n    ## [61] \"var_roll_forearm\"         \"avg_pitch_forearm\"       \r\n    ## [63] \"stddev_pitch_forearm\"     \"var_pitch_forearm\"       \r\n    ## [65] \"avg_yaw_forearm\"          \"stddev_yaw_forearm\"      \r\n    ## [67] \"var_yaw_forearm\"\r\n\r\nNow get columns that are actually **empty** (actually similar to na - more than **95% empty**), display all of them and finally perform cleaning on dataset:\r\n\r\n    ## [1] \"Number of Empty columns dropped = 33\"\r\n\r\n    ##  [1] \"kurtosis_roll_belt\"      \"kurtosis_picth_belt\"    \r\n    ##  [3] \"kurtosis_yaw_belt\"       \"skewness_roll_belt\"     \r\n    ##  [5] \"skewness_roll_belt.1\"    \"skewness_yaw_belt\"      \r\n    ##  [7] \"max_yaw_belt\"            \"min_yaw_belt\"           \r\n    ##  [9] \"amplitude_yaw_belt\"      \"kurtosis_roll_arm\"      \r\n    ## [11] \"kurtosis_picth_arm\"      \"kurtosis_yaw_arm\"       \r\n    ## [13] \"skewness_roll_arm\"       \"skewness_pitch_arm\"     \r\n    ## [15] \"skewness_yaw_arm\"        \"kurtosis_roll_dumbbell\" \r\n    ## [17] \"kurtosis_picth_dumbbell\" \"kurtosis_yaw_dumbbell\"  \r\n    ## [19] \"skewness_roll_dumbbell\"  \"skewness_pitch_dumbbell\"\r\n    ## [21] \"skewness_yaw_dumbbell\"   \"max_yaw_dumbbell\"       \r\n    ## [23] \"min_yaw_dumbbell\"        \"amplitude_yaw_dumbbell\" \r\n    ## [25] \"kurtosis_roll_forearm\"   \"kurtosis_picth_forearm\" \r\n    ## [27] \"kurtosis_yaw_forearm\"    \"skewness_roll_forearm\"  \r\n    ## [29] \"skewness_pitch_forearm\"  \"skewness_yaw_forearm\"   \r\n    ## [31] \"max_yaw_forearm\"         \"min_yaw_forearm\"        \r\n    ## [33] \"amplitude_yaw_forearm\"\r\n\r\n### Predictors variance and correlation analysis\r\n\r\nThe next step in building our model, after basic cleaning is to analyze again the predictors variance using `nearZeroVar`. Then we will sort and display the predictors with least variance and also display all factor variables and their summary omitting the label `classe`. Finally we drop the factor variables with near-zero variance:\r\n\r\n    ## [1] \"Near zero variance table:\"\r\n\r\n    ##                      freqRatio percentUnique zeroVar   nzv\r\n    ## new_window           47.330049    0.01019264   FALSE  TRUE\r\n    ## classe                1.469581    0.02548160   FALSE FALSE\r\n    ## total_accel_belt      1.063160    0.14779329   FALSE FALSE\r\n    ## total_accel_dumbbell  1.072634    0.21914178   FALSE FALSE\r\n    ## total_accel_arm       1.024526    0.33635715   FALSE FALSE\r\n    ## gyros_belt_y          1.144000    0.35164611   FALSE FALSE\r\n\r\n    ## [1] \"Factors variables to be dropped:\" \"new_window\"\r\n\r\n    ##  new_window \r\n    ##  no :19216  \r\n    ##  yes:  406\r\n\r\nVariable correlation analysis\r\n-----------------------------\r\n\r\nNow analyze the predictor variables correlation in order determine if we have very high correlation. We do this by calculating correlation matrix with `cor(exploreData[,setdiff(colnames(exploreData),c(\"classe\"))])` (excluding label `classe` column):\r\n\r\n    ##  [1] \"Highly correlated predictor variables:\"\r\n    ##  [2] \"accel_belt_z\"                          \r\n    ##  [3] \"roll_belt\"                             \r\n    ##  [4] \"accel_belt_y\"                          \r\n    ##  [5] \"accel_arm_y\"                           \r\n    ##  [6] \"total_accel_belt\"                      \r\n    ##  [7] \"accel_dumbbell_z\"                      \r\n    ##  [8] \"accel_belt_x\"                          \r\n    ##  [9] \"pitch_belt\"                            \r\n    ## [10] \"magnet_dumbbell_x\"                     \r\n    ## [11] \"accel_dumbbell_y\"                      \r\n    ## [12] \"magnet_dumbbell_y\"                     \r\n    ## [13] \"accel_arm_x\"                           \r\n    ## [14] \"accel_dumbbell_x\"                      \r\n    ## [15] \"accel_arm_z\"                           \r\n    ## [16] \"magnet_arm_y\"                          \r\n    ## [17] \"magnet_belt_z\"                         \r\n    ## [18] \"accel_forearm_y\"                       \r\n    ## [19] \"gyros_dumbbell_x\"                      \r\n    ## [20] \"gyros_forearm_y\"                       \r\n    ## [21] \"gyros_dumbbell_z\"                      \r\n    ## [22] \"gyros_arm_x\"\r\n\r\nDue to high correlation between variable we might need to apply PCA later or use a feature selection methods available in the `caret` package\r\n\r\n### Recursive Feature Elimination Step\r\n\r\nFinally in data cleaning and pre-processing stage we analyze the actual variables importance based on a trained model and obtain a automatic features selection model based on `rfe` available within `caret` package.We will get two samples of our data and then train two different `rfe` models based on random forests followed by plot view/analysis on both. First plot of number of features vs cross-validation accuracy:\r\n\r\n![](fig1.png)\r\n\r\nAnd now the second plot of predictor variables quantity vs prediction accuracy:\r\n\r\n![](fig2.png)\r\n\r\nBased on the two plots it is obvious that best number of predictors is between 5 and 12. Now we combine our findings in order to obtain a final list of predictor variables:\r\n\r\n    ## [1] \"FINAL PREDICTORS :\" \"roll_belt\"          \"num_window\"        \r\n    ## [4] \"magnet_dumbbell_z\"  \"pitch_forearm\"      \"magnet_dumbbell_y\"\r\n\r\nTRAINING AND TESTING MODELS\r\n---------------------------\r\n\r\nFinally we can now prepare training, cross-validation and test datasets (training dataset 60%, crossval dataset 20%, testing dataset 20%), but first having the list of all `dropped_columns` we can either apply it to `finalData` or we could use the automated selection of predictor variables. Based on this, we have a few special meta-parameters for customizing our model: - `useAutomaticPredictors` controls if we use or not the short list of predictors generated by **Recursive Feature Elimination** - `usePCA` controls if we use or not dimensionality reduction preprocessing based on **Principal Components Analysis**\r\n\r\n``` r\r\nuseAutomaticPredictors = TRUE\r\nusePCA = FALSE\r\n\r\nif (useAutomaticPredictors){\r\n  good_columns <- c(final_predictors,c(\"classe\"))\r\n  pred_columns <- final_predictors \r\n}else{\r\n  good_columns <- setdiff(colnames(finalData), dropped_columns)\r\n  pred_columns <- setdiff(good_columns, c(\"classe\"))\r\n  \r\n}\r\n\r\nfinalData <- finalData[good_columns]\r\ninTraining <- createDataPartition(finalData$classe, p=0.6, list=FALSE)\r\ntrainingStd <- finalData[inTraining,]\r\ntestdataStd <- finalData[-inTraining,]\r\ninVal <- createDataPartition(testdataStd$classe, p=0.5, list=FALSE)\r\ncrossvalStd <- testdataStd[inVal,]\r\ntestingStd <- testdataStd[-inVal,]\r\n##\r\n## Now the PCA pre-processing stage (if needed)\r\n##\r\nif (usePCA)\r\n{\r\n  PCA.model <- preProcess(trainingStd[pred_columns],method=\"pca\", thresh=0.95)\r\n  training <- predict(PCA.model, trainingStd)\r\n  crossvalidation <- predict(PCA.model,crossvalStd )\r\n  testing <- predict(PCA.model, testingStd)  \r\n} else\r\n{\r\n  training <- trainingStd\r\n  crossvalidation <- crossvalStd\r\n  testing <- testingStd\r\n}\r\n```\r\n\r\nMulti-model cross-validation testing\r\n------------------------------------\r\n\r\nNow I train several different models, analyze them and then and then choose the best model based on best cross validation score. So first stage is timed training for each proposed model and cross-validations. Keep all accuracy values in vectors then combine in dataframe to finally display.\r\n\r\n``` r\r\nAll.Methods <- c(\"lda\",\"rpart\",\"knn\",\"lvq\",\"xgbTree\")\r\nnr_models <- length(All.Methods)\r\nCross.Accuracy <- c()\r\nTraining.Time <- c()\r\nbestAccuracy <- 0 \r\n\r\nfor (c_model in 1:nr_models){\r\n  \r\n  methodName <-  All.Methods[c_model]\r\n  print(paste0(\"Training \",methodName,\"...\"))\r\n  tmr_start <- proc.time()\r\n  curr.model <- train(classe ~ .,\r\n                      data = training,\r\n                      method = methodName)\r\n  tmr_end <- proc.time()\r\n  print(paste0(\"Done training \",methodName,\".\"))  \r\n  Training.Time[c_model] = (tmr_end-tmr_start)[3]\r\n  \r\n  preds<- predict(curr.model,crossvalidation)\r\n\r\n  cfm <- confusionMatrix(preds,crossvalStd$classe)\r\n  Cross.Accuracy[c_model] <- cfm$overall['Accuracy']\r\n  \r\n  if(bestAccuracy < Cross.Accuracy[c_model]){\r\n    best.model <- curr.model\r\n    bestAccuracy <- Cross.Accuracy[c_model]\r\n  }\r\n  \r\n}\r\n```\r\n\r\n    ## [1] \"Training lda...\"\r\n\r\n    ## Loading required package: MASS\r\n\r\n    ## [1] \"Done training lda.\"\r\n    ## [1] \"Training rpart...\"\r\n\r\n    ## Loading required package: rpart\r\n\r\n    ## [1] \"Done training rpart.\"\r\n    ## [1] \"Training knn...\"\r\n    ## [1] \"Done training knn.\"\r\n    ## [1] \"Training lvq...\"\r\n\r\n    ## Loading required package: class\r\n\r\n    ## [1] \"Done training lvq.\"\r\n    ## [1] \"Training xgbTree...\"\r\n\r\n    ## Loading required package: xgboost\r\n\r\n    ## Loading required package: plyr\r\n\r\n    ## [1] \"Done training xgbTree.\"\r\n\r\nAnd the custom constructed summary of the training and cross-validation process\r\n-------------------------------------------------------------------------------\r\n\r\n    ##   All.Methods Cross.Accuracy Training.Time\r\n    ## 1         lda      0.3907724          0.91\r\n    ## 4         lvq      0.5281672         46.55\r\n    ## 2       rpart      0.5388733          1.28\r\n    ## 3         knn      0.9548815          7.24\r\n    ## 5     xgbTree      0.9994902        175.94\r\n\r\n### Almost there !\r\n\r\nNow that we have our final model lets apply it on testing dataset and then display confusion matrix so we can visually compare test result with previous cross validation one. We could also use a random forest or other classifier to ensemble to 2 or 3 predictors. Nevertheless this is not needed for this particular exercise as _**top two predictors achieved over 95% accuracy**_ with the best one constantly over 99%.\r\n\r\n    ## [1] \"Predicting with best predictor: xgbTree\"\r\n\r\n    ## Confusion Matrix and Statistics\r\n    ## \r\n    ##           Reference\r\n    ## Prediction    A    B    C    D    E\r\n    ##          A 1113    0    0    0    0\r\n    ##          B    0  759    0    0    0\r\n    ##          C    1    0  684    0    0\r\n    ##          D    0    0    0  643    0\r\n    ##          E    2    0    0    0  721\r\n    ## \r\n    ## Overall Statistics\r\n    ##                                           \r\n    ##                Accuracy : 0.9992          \r\n    ##                  95% CI : (0.9978, 0.9998)\r\n    ##     No Information Rate : 0.2845          \r\n    ##     P-Value [Acc > NIR] : < 2.2e-16       \r\n    ##                                           \r\n    ##                   Kappa : 0.999           \r\n    ##  Mcnemar's Test P-Value : NA              \r\n    ## \r\n    ## Statistics by Class:\r\n    ## \r\n    ##                      Class: A Class: B Class: C Class: D Class: E\r\n    ## Sensitivity            0.9973   1.0000   1.0000   1.0000   1.0000\r\n    ## Specificity            1.0000   1.0000   0.9997   1.0000   0.9994\r\n    ## Pos Pred Value         1.0000   1.0000   0.9985   1.0000   0.9972\r\n    ## Neg Pred Value         0.9989   1.0000   1.0000   1.0000   1.0000\r\n    ## Prevalence             0.2845   0.1935   0.1744   0.1639   0.1838\r\n    ## Detection Rate         0.2837   0.1935   0.1744   0.1639   0.1838\r\n    ## Detection Prevalence   0.2837   0.1935   0.1746   0.1639   0.1843\r\n    ## Balanced Accuracy      0.9987   1.0000   0.9998   1.0000   0.9997\r\n\r\nNow finally apply best model on unseen observation. _Note: xGBoost model constantly achieved over 98% accuracy on all crossval/tests_:\r\n--------------------------------------------------\r\n\r\n    ## [1] \"Now predicting unseen observations with: xgbTree\"\r\n\r\n    ##  [1] B A B A A E D * * * * * * * * * * * * *\r\n    ## Levels: A B C D E\r\n\r\n_**Please note I partially masked the final prediction in order not to bias other students and/or violate Coursera Honor Code.**_",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}